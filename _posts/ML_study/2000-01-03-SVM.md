---
layout: post
title: SVM
description: Support Vector Machine
image:
use_math: true
ml: True
---

> - Machine Learning (머신러닝)
> - Supervised Learning (지도학습)
> - Binary Classification (이진 분류)



### Support Vector Machine (SVM) 특성

> - 연속적 이진분류를 통해 다중분류 가능
> - Margin을 이용한 분류 기법
> - 과잉적합 문제 작음, 일반화 특성 우수
> - Kernel Trick을 이용한 비선형 분리



### Support Vector Machine (SVM)의 작동 방식

SVM은 데이터를 구분하는 경계를 이용하여 분류를 수행한다. 이때 데이터를 구분하는 경계는 좌우 혹은 상하로 적절한 이동범위(Margin)을 갖게되며, 이 Margin으로 인하여 일반화 특성이 좋아지게된다.

SVM의 핵심은 초평면(Margin이 갖는 범위)을 최대화 하는데에 있으며,  분류 기준이라는 조건하에 이를 수행하기 위하여 라그랑제 승수법(단 $\lambda$값을 알 수 없음으로 Dual Lagrange)을 이용한다.



### SVM - Formula

#### <선형분리가 가능한 경우>(그림참고)

<center><img src="{{ "/assets/images/SVM/SVM_margin.png" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

Margin을 계산 라기 위해서는 먼저 위 그림의 검은 선(경계)에 해당하는 식을 세워야한다. ($\cdot$ : 내적)



> $$
> \begin{align*}
> 
> w_{(1)}x_{(1)} + w_{(2)}x_{(2)} + b &= 0\\
> w \cdot X &= 0
> 
> \end{align*}
> $$



경계에 대한 식을 이용하여 보라색 선을 수식화 한다.

극값(경계가 위, 아래로 이동했을 때 만나게 되는 최초의 Value)은 k로 표현한다 



>$$
>\begin{align*}
>
>upper(+1): \qquad w \cdot X +b &= k \quad (k>0)\\
>lower(-1): \qquad w \cdot X +b &= -k
>
>\end{align*}
>$$



이 두식을 열립하여 전개할 경우 다음과 같은 식을 얻을 수 있다. ($\|\|x\|\|$ :  x의 노름)



>$$
>\begin{align*}
>
>(w \cdot X_{(upper)} + b) - (w \cdot X_{(lower)} +b) &= k -(-k)\\
>w(X_{(upper)} - X_{(lower)})&= 2k\\
>X_{(upper)} - X_{(lower)} &= \frac{2k}{||w||}
>
>\end{align*}
>$$



이때 $X_{(upper)} - X_{(lower)}$는 그림에서의 마진 즉 d가 되기에 최종적으로 다음과 같은 식을 얻는다.



>$$
>d = \frac{2}{||w||}
>$$



위 식을통해 SVM의 목적인 Margin의 최대화(d의 최대화)를 위하여 w를 최소화 시키게 된다.

즉 최대값을 구하는 위의 식을 역수를 취해 최소화 하는 문제로 바꾼게되면 다음과 같은 목표함수를 얻을 수 있다.



>$$
>argmin\frac{1}{2} ||w||^{2}
>$$



이와 더불어 라그랑제 승수법을 이용하기 위하여 제약조건(constraint)를 다음과 같이 정의 한다.



> $$
> y_i(w \cdot x_i+b) \ge k \;\;\;\;\;\; (i = 1,2,3\;...\;N)
> $$



목표함수와 제약조건을 이용하여 라그랑제 함수를 구성한다.



>$$
>L_{primal} = \frac{1}{2} ||w||^2-\sum_{i=1}^{N}\lambda\{y_i(w\cdot x_i+b)-k\}
>$$



라그랑제 함수를 구성하였지만, SVM은 등식 제약 조건부 최적화 문제가아닌 부등식 제약 최적화 문제이다. 

따라서 KKT(Karush-Kuhn-Tucker)조건을 추가로 이용한다. (KKT조건은 필요조건, 즉 반드시 만족 해야한다)

- 라그랑제 함수에서 라그랑제 승수($\lambda$)를 제외한 w와 b로 편미분한 식이 0이 되어야 한다.
- 모든 $\lambda$는 0보다 크거나 같아야한다.



>$$
>\begin{align*}
>
>\frac{\partial L_{primal}}{\partial w} = 0\; &\to \; w = \sum_{i=1}^N\lambda_iy_ix_i\;\;\;\;\;(\lambda_i \ge0)\\
>\frac{\partial L_{primal}}{\partial b}= 0\; &\to \; \sum_{i=1}^N \lambda_iy_i = 0\;\;\;\;\;(\lambda_i \ge0)
>
>\end{align*}
>$$



 $lambda$값을 알아내기 위하여 기존 Primal Lagrange를 Dual Lagrange로 변환한다.



> $$
> D_{dual} = \sum_{i=1}^N\lambda_i - \frac{1}{2}\sum_{i,j}\lambda_i\lambda_jy_iy_jx_i\cdot x_j
> $$



이후 Dual Lagrange에서 $lambda$를 구한 후, 위의 KKT조건 1번식으로 w를 계산하고, 제한조건 하에서 b를 계산하게된다.

Dual Lagrange를 계산할시데이터가 많은 경우 Quadratic Optimization을 이용해야 하나, 직관적인 이해를 위해 훈련데이터가 적은 경우로 단순화시켜 Decision Boundary를 찾으면 다음과 같다.

<center><img src="{{ "/assets/images/SVM/SVM_dual_L.png" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

- N = 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i = {1,2,3}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;j = {1,2,3}



>$$
>\begin{align*}
>L_{dual} &= \sum^N_{i=1}\lambda_i - \frac{1}{2}\sum_{i,j}\lambda_i\lambda_jy_iy_jx_i\cdot x_j\\
>&= \lambda_1+\lambda_2+\lambda_3 - \frac{1}{2} \times(\lambda_1\lambda_1y_1y_1x_1x_1 + \lambda_1\lambda_2y_1y_2x_1x_2 + \lambda_1\lambda_3y_1y_3x_1x_3 \\
>&\qquad\qquad\qquad\qquad\qquad\;\; 
>+\lambda_2\lambda_1y_2y_1x_2x_1 + \lambda_2\lambda_2y_2y_2x_2x_2 + \lambda_2\lambda_3y_2y_3x_2x_3 \\
>&\qquad\qquad\qquad\qquad\qquad\;\; 
>+\lambda_3\lambda_1y_3y_1x_3x_1 +
>\lambda_3\lambda_2y_3y_2x_3x_2 + \lambda_3\lambda_3y_3y_3x_3x_3)\\
>&= \lambda_1 + \lambda_2 + \lambda_3 - \frac{1}{2}(10\lambda_1^2+8\lambda_1\lambda_2-4\lambda_1\lambda_3+8\lambda_2\lambda_1+8\lambda_2^2-4\lambda_2\lambda_3-4\lambda_3\lambda_1-4\lambda_3\lambda_2+2\lambda_3^2)\\
>&= \lambda_1+\lambda_2+\lambda_3-\frac{1}{2}(6\lambda_1^2+16\lambda_1\lambda_2-8\lambda_1\lambda_3+8\lambda_2^2-8\lambda_2\lambda_3+2\lambda_3^2)\quad \\
>&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad(KKT_{2번}: \lambda_1+\lambda_2-\lambda_3=0)\\
>&=2\lambda_1+2\lambda_2-2\lambda_1\lambda_2-\lambda_2^2\\
>\\
>\frac{\partial L_{dual}}{\partial \lambda_1} &= 1-\lambda_2 = 0\\
>\frac{\partial L_{dual}}{\partial \lambda_2} &= 1-\lambda_1-\lambda_2 = 0
>\\
>\\
>\lambda_1 &= 0 \qquad \lambda_2 = 1 \qquad \lambda_3 = 1
>\\
>\\
>w &= \lambda_1y_1x_1+\lambda_2y_2x_2 + \lambda_3y_3x_3 \qquad (KKT_{1번} :w = \sum_{i=1}^N\lambda_iy_ix_i)\\
>w&=(2,2)-(1,1) = (1,1)\\
>\\
>x_2&:w\cdot x_2+b > 0 \to 4+b>0\\
>x_3&:w\cdot x_3+b < 0 \to 2+b<0\\
>\\
>-4&<b<-2 \to b= -3
>\\
>\\
>Decision\;Boundary &: x_{(1)}+x_{(2)}-3 = 0
>\end{align*}
>$$



위의 방식처럼 분류를 진행할경우 인공신경망등의 분류 방식보다 일반화 특성이 우수하다.

- 인공신경망 등의 분류 방식은 지정된 오류 이하로 학습되면 그 지점에서 Decision Boundary를 결정한다. 하지만 이때의 Decision Boundary는 여러 구분선 중 하나이다(위의 예시상에선 -4 ~ -2사이 중 하나 ).





#### <선형분리가 불가능한 경우>(그림 참고) - Soft margin

다음과 같은 경우는 직선으로 분리가 불가능하다. 

<center><img src="{{ "/assets/images/SVM/SVM_non_linear.png" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

따라서 Slack 변수($\xi$)를 추가하여 경계에 대한 제한 조건을 완화한다.



> $$
> \begin{align*}
> upper(+1): \qquad w \cdot x_i +b &\geq 1-\xi_i\\
> lower(-1): \qquad w \cdot x_i +b &\leq -1+\xi_i \quad (\xi_i>0)
> \end{align*}
> $$




- 위의 그림에서 점 A와 B는 첫 예시와 같이 경계를 계산할 경우 경계를 벗어나 있다.
- 하지만 $\xi$를 이용하여 제약조건을 완화 할 시 제한조건에 위배되지 않을 수 있다. 
  - $y_A=-1,\;\xi_A=10$ 이라면 $w\cdot x_A +b \leq9$ 가 됨으로 제한조건에 위배되지 않는다.



제한 조건을 완화한 것을 반영하여 목적함수를 정의한다. (penalty 항 추가)

- C 는 모형의 복잡도를 조절하는 상수로 분석가가 결정한다. (예외를 얼마나 고려할지, C = 0이라면 예외를 허용하지 않음)



> $$
> argmin\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i
> $$



이후 기존과 같이 라그랑제 함수(penalty항이 추가된)를 구성한다. ($\mu$ : Lagrange 승수, slack변수 추가로인하여 추가됨)



> $$
> L_{primal} = \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\lambda_i\{y_i(w\cdot x_i+b)-1+\xi_i\}-\sum_{i=1}\mu_i\xi_i
> $$



슬랙변수를 추가 하였기 때문에 기존의 2가지 KKT조건 외에 한가지가 추가된다.



> $$
> \frac{\partial L_{primal}}{\partial\xi_i} = 0 \;\;\to\;\; \lambda_i + \mu_i = C \;\;\to\;\; 0 \leq \lambda_i \leq C
> $$
>
> 




Dual Lagrange는 기존과 동일함으로(제한 조건만 다름) 선형분리가 가능한 경우와 동일하게 $\lambda$를 구할수 있고, 이를 이용하여 w를 구할 수 있게된다.



#### <선형분리가 불가능한 경우>(그림 참고) - Kernel Trick

- 아래의 그림에서 직선 A보다 곡선 B로 구분하는 것이 타당하다. $\to$ 비선형 SVM
- 원래의 공간(좌측)에 있는 데이터들을 선형분리가 가능한 좌표 공간(우측)으로 변환 시키면 선형으로 구분할 수 있다. (공간을 변형시킨다, 물이 반 담겨있는 투명한 통을 위에서 보면 다 차있는데, 옆에서 보면 반만 차있는 개념)
- 변환된 공간에서, 선형분리가 가능한 경우와 동일하게 w와 b를 결정한다.
- 선형 분리가 가능한 변환 함수$\phi (x)$를 결정하는것이 중요하며, 이론적으로는 무한차원으로 보낼경우 선형분리가 가능하다.
- 적절한 변환함수를 알아도, 차원이 높아지게되면 많은 계산량이 요구된다. (차원의 저주)
- 변화된 공간에서 Dual Lagrange는 변환 함수 자체가 아닌, 변환 함수들간의 내적만 알면 되기에 원래 공간에서 데이터들의 내적을 구하고, 이것을 변환 공간으로 보낼 수 있다면 계산량을 줄일 수 있다.
  - $K(u,v) = \phi(u) \cdot \phi(v) = g(u \cdot v) \to K(u,v) \;\;\; : \;\;Kernel\;function$

<center><img src="{{ "/assets/images/SVM/Kernel_trick.png" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>



>$$
>L_{dual} = \sum_{i=1}^N \lambda_i -\frac{1}{2}\sum_{i,j}\lambda_i\lambda_jy_iy_j\phi(x_i) \cdot \phi(x_j)
>$$



- 변환 함수들의 내적 ($\phi(x_1) \cdot \phi(x_2)$)을 계산하면 $L_{dual}$를 계산할 수 있음. 원해의 공간에서 $x_1$과 $x_2$의 내적을 구한 후 결과를 변환된 공간으로 보냄



### Practice (python)

[Iris data (linear)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/SVM/svm(linear-iris).py)

[Iris data (rbf-less feature)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/SVM/svm(rbf-iris).py)

[Iris data (rbf-all feature)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/SVM/svm(rbf-iris-all).py)
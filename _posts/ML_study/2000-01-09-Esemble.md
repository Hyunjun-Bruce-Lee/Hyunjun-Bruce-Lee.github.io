---
layout: post
title: Esemble
description: Voting, Bagging, Boosting
image:
use_math: true
ml: True
---

### Esemble-Voting (앙상블-보팅)

> - 약학습기 연결을 통한 집단 지성
> - 동일 데이터 이용
> - 서로 다른 복수개의 모델

 ### Voting

Voting은 여러 모델의 조합으로 구성되는 앙상블 방법 중 한가지로, 여러 모델의 예측값에 대한 투표를 통하여 최종 예측값을 반환하는 기법이다.

Voting 모델을 구성하기 위해서는 먼저 Voting 모델을 구성할 여러 모델에 대한 정의를 하여야 한다. 동일한 데이터셋을 이용하여 정의된 여러 모델을 훈련하게 되고, 모델들의 예측값들을 모아 투표를 통하여 최종 예측 값을 얻게된다(회귀 문제일 경우 평균값).

<center><img src="{{ "/assets/images/Ensemble/Ensemble_voting.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

Voting 모델을 구성하는 모델이 자체적인 Hyper Parameter를 갖는경우 이에대한 설정이 선행되어야 한다.



### Hard Voting? Soft Voting?

최종 예측값을 도출하는 투표(Voting)방법은 다음과 같은 2가지가 있다.

- **Hard Voting**
  - N개의 모델에서 추정한 값들 중 가장 많은 것으로 판정(다수결)
- **Soft Voting**
  - N개의 모델에서 추정한 각 값의 확률값의 평균(or 합)을 계산한후 가장높은 값을 갖는 Label로 판정



### Practice (Python)

[IRIS data](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/Esemble/VoteClassifier.py)







### Esemble-Bagging (앙상블-배깅)

> - 약학습기 연결을 통한 집단 지성
> - 단순복원 임의 추출(Bootstrap)
> - 서로 같은 복수개의 모델

 ### Bagging

Bagging은 앙상블 기법중 한가지인 Voting과 매우 유사하지만 같은 알고리즘을 이용한 복수의 모델을 사용하는점, Bootstrap이라불리는 단순복원 임의 추출을 이용하여 각 모델의 훈련데이터를 구성하는 점이 다르다. (Bagging의 대표적인 모델로는 Random Forest가 있다. - Decision Tree를 이용한 Bagging 기법)

<center><img src="{{ "/assets/images/Ensemble/Ensemble_bagging.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>



### Practice (Python)

[IRIS,Wine data(bagging with voting)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/Esemble/Bagging.py)

[IRIS (Random Forest)]()[https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/Esemble/Bagging(RandomForest).py]





### Esemble-Boosting (앙상블-배깅)

> - 약학습기 연결을 통한 집단 지성
> - 선택적, 확률기반 훈련 데이터



### Boosting

Boosting은 Bagging과 동일한 방식으로 훈련과 예측이 이루어진다. 단, Bagging과는 다르게 단순복원 임의 추출을 통해 각 모델의 훈련데이터를 선별하는 것이 아닌, 이전 모델이 잘 분류 하지 못하지 못한 데이터들의 선별 확률을 증가 시켜 모델의 훈련데이터를 선별한다.

단, 잘 분류하지 못한 데이터에 치중하는 경향이 있기에, 오버피팅이 발생하기 쉬우며, 순차적으로 학습을 진행해야 하기 때문에 다른 Esemble기법보다 시간이 오래걸린다는 단점이있다.

<center><img src="{{ "/assets/images/Ensemble/Ensemble_boosting.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>



최초의 Boosting기반 알고리즘인 Ada Boosting은 T개의 모델이 있을때 다음과 같이 순차적으로 각모델에 대한 훈련 데이터를 샘플링한다.
$$
\begin{align*}
\epsilon_0&=\sum_{i=1}^{N}w_i^{(0)} \cdot I(y_i \neq \hat{y}_i)\qquad\leftarrow\;^{오분류율}_{I\; : \; indicator(0\;or\;1)}\\
\alpha_0&= \frac{1}{2}log \big(\frac{1-\epsilon_0}{\epsilon_0}\big)\\
w_i^{(1)} &= \frac{w_i^{(0)}}{Z} \cdot \big\{^{exp(-\alpha_0)\;\leftarrow\;y_i = \hat{y}_i}_{exp(\alpha_0)\;\leftarrow\;y_i \neq\hat{y}_i}\qquad \leftarrow\;^{잘\;분류한\;샘플의\;가중치는\;낮추고,}_{잘못분류한\;샘플의\;가중치\;증가}\\
\qquad\\
&\qquad\vdots \qquad T회\; 반복\\
\qquad\\
w_i^{(T)} &= \frac{w_i^{(T-1)}}{Z} \cdot \big\{^{exp(-\alpha_{(T-1)})\;\leftarrow\;y_i = \hat{y}_i}_{exp(\alpha_{(T-1)})\;\leftarrow\;y_i \neq\hat{y}_i}
\end{align*}
$$




### Practice (Python)

[IRIS data(Ada Boosting)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/Esemble/AdaBoost.py)


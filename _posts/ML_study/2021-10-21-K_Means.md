---
: post
title: K-Means
description: K-Means
image:
use_math: true
ml: True
---



> - Machine Learning (머신러닝)
> - Unsupervised Learning (비 지도학습)
> - Clustering (군집화)



### K-Means 특성

> - 중점(평균)을 이용한 군집화 기법
> - 사전 정의 군집 개수 (Elbow & Silhouette)
> - EM 알고리즘 (기댓값 최대화 알고리즘)



### K-Means의 작동 방식

초기 중점을 랜덤하게 부여한 후, 중점들과 학습 데이터간 거리의 평균이 최소화 되도록 중점을 이동시킨다. 중점이 이동하지 않게 되면 학습을 종료한다. 이후 새로운 데이터가 발생하게되면, 학습을 통하여 설정된 중점들과의 거리를 계산하여 거리가 최소인 중점의 군집으로 분류한다.



### K-Means 작동 상세(EM 알고리즘)

- k-means 에서는 파라메터 벡터가 2개(r,u)임으로 MLE(Maximum Likelihood)로 최적화가 어렵다. 따라서 중점을 할당한 후 할당된 중점까지의 거리 합을 최소화 하기 위해 EM 알고리즘을 이용한다.

- EM 알고리즘은 E-step과 M-step으로 구분되며 E-step에서는 중점을 할당하고, M-step에서는 할당된 중점까지의 거리를 최소화 한다.

- r(Assignment)이 {0,1}이므로 hard clustering의 형태이다. 이때 r이 연속형(확률)이라면 GMM모형으로의 확장이 필요하다.



K-Means는 다음과 같은 식을 기반으로 최적의 중점을 찾아간다.

<center><img src="{{ "/assets/images/K_means.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

> $$
> \begin{align*}
> J =\;\; 중점까지 거&리 제곱 합\;\;r_{x\mu} = relation\;between\;x\;and\;\mu\\ if&\;\;\;\;r_{x\mu} = 1,\;x가\;중점\;\mu에\;할당됨
> \;\\
> \;\\
> J\;=\;\;& r_{11}||x_1-\mu_1||^2 + r_{12}||x_1-\mu_2||^2 +\\ 
> &r_{21}||x_2-\mu_1||^2 + r_{22}||x_2-\mu_2||^2 + \\
> &r_{31}||x_3-\mu_1||^2 + r_{32}||x_3-\mu_2||^2 + \\
> &r_{41}||x_4-\mu_1||^2 + r_{42}||x_4-\mu_2||^2 + \\
> \;\\
> J =\;\;& \sum_{n=1}^N \sum_{k=1}^Kr_{nk}||x_n-\mu_k||^2 
> \end{align*}
> $$

**Step-1 (E-step)**

- E-step 에서는 거리를 기준으로 데이터 x를 중점에 할당한다. 

> $$
> r_{11}\;=\;1,\;\;r_{12}\;=\;0\\
> r_{21}\;=\;1,\;\;r_{22}\;=\;0\\
> r_{31}\;=\;0,\;\;r_{32}\;=\;1\\
> r_{41}\;=\;0,\;\;r_{42}\;=\;1\\
> $$

- 위 그림을 기준으로 $x_1$과 $x_2$는 $\mu_1$에, $x_3$과 $x_4$는 $\mu_2$에 할당되었다.
- 따라서 실제 중점까지 거리 제곱합은 다음과 같게 된다.

> $$
> \begin{align*}
> J\;=\;\;& 1\times||x_1-\mu_1||^2 + 0\times||x_1-\mu_2||^2 +\\ 
> &1\times||x_2-\mu_1||^2 + 0\times||x_2-\mu_2||^2 + \\
> &0\times||x_3-\mu_1||^2 + 1\times||x_3-\mu_2||^2 + \\
> &0\times||x_4-\mu_1||^2 + 1\times||x_4-\mu_2||^2 + \\
> \;\\
> J =\;\; ||x_1-\mu_1|&|^2 + ||x_2-\mu_1||^2+
> ||x_3-\mu_2||^2 + ||x_4-\mu_2||^2
> \end{align*}
> $$

**Step-2 (M-step)**

- M-step에서는 E-step에서 정의된 $J$를 최소화 시키기 위하여 편미분을 진행한다.

> $$
> \begin{align*}
> 
> \frac{\partial J}{\partial \mu_k}=&-2\sum_{n=1}^N r_{nk}||x_n-\mu_k|| = 0
> \end{align*}
> $$

- 위 식을 전개할시 다음과 같은 식을 얻을 수 있다.

> $$
> \sum_{n=1}^Nr_{nk}x_n-\sum_{n=1}^Nr_{nk}\mu_k = 0\\
> \;\\
> \;\\
> \mu_k = \frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}
> $$

- 이떄 $\mu_k$에 해당하는 지점이 K번째 중점의 이동지점이다.
- 위의 계산을 n회 반복하여 더이상 중점이 이동하지 않을 때까지 진행하여 최종 중점을 결정하게 된다.



### K-means에서의 K

K-means에서 K는 데이터를 분할 할 군집의 개수를 의미한다. K는 모델 내에서 자동으로 결정되지 않기에, 분석가가 사전에 적정 K를 탐색하여 정하여야 한다.

하지만 K를 정함에 있어 K-means는 비지도 학습이기에 label혹은 target값이 존재하지 않아 군집화 결과와 비교하여 오차를 정의할 만한 값이 존재하지 않는다. 

이에 따라 K값을 결정하기 위해 다음과 같은 2가지 방법(Elbow, Silhouette)을 이용한다. 

**Elbow**

Elbow 방법의 경우 "군집화가 잘 진행되었다면, 각 중점과 해당 군집내의 데이터들간의 거리 합(distortion or inertia)이 작을 것이다"라는 아이디어에서 출발한다.

단 이때 군집화를 진행함에있어, 5개의 데이터를 5개로 분류 하는것과 같이 너무 세세하게 군집화를 하게되면 군집화의 의미가 소실되기에 distortion의 감소폭이 갑자기 작아지는 지점의 K를 선택하는 것이 일반적이다.

<center><img src="{{ "/assets/images/K_menas_elbow.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

**Silhouette**

Sihouette방법의 경우 "군집화가 잘 진행되었다면, 각 군집내의 데이터간 거리가 가깝고, 각 군집간 거리가 멀것이다"라는 아이디어에서 출발한다.

이때 각 군집내 데이터간 거리의 경우 Cohesion(응집도)라고 표현하며,  데이터 x와 x가 속한 클러스터내의 다른 데이터들간의 평균 거리를 이용한다.

각군집간 거리의 경우 Separation(분리도)라고 표현하며, 데이터 x와 x가 속하지 않은 인접 클러스터 내의 데이터들과의 평균 거리를 이용한다.

<center><img src="{{ "/assets/images/K_means_silhouette.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

이를토대로 다음과 같은 식을 통해 실루엣 점수를 얻을 수 있다.

> $$
> Silhouette\;Score = \frac{1}{K}\sum_i^K \left(\frac{Separation\;-\;Cohesion}{max(Separation,Cohesion)}\right)
> $$

실루엣 점수가 높다는 것은 Separation이 Cohesion보다 높은 경우 이기에 타 군집과의 거리가 크다라는 의미임으로 군집화가 잘 진행되었다고 평가할 수 있다. 이와 반대로 Cohesion이 Separation보다 클경우 군집내의 응집도가 군집간 거리보다 크다는 의미임으로 실루엣점수가 낮게 나오게된다.

 

### K-means++

K-means의 경우 기존 임의의 지점을 초기 중점으로 이용하기에 local optimum지점에 빠질 가능성이 있다. K-means에서는 이러한 문제를 해결하기 위하여 초기중점을 여러번 다르게 설정하며 최종중점을 선택한다.(code상 n_init옵션)

K-means++의 경우 초기중점을 랜덤하게 설정함에 따라 발생하는 local optimum문제를 좀더 합리적으로 해결하기위한 방법이다. 

K-means++는 임의의 데이터 한개를 선택하여 첫번째 초기 중점으로 설정하고, 해당 첫번째 중점에서 다른 데이터까지의 거리를 계산하여 거리에 대한 확률 분포를 만든 후, 확률분포상에서 1개를 샘플링하여 2번째 중점으로 설정한다.

위의 작업을 K개의 중점 모두가 설정될때까지 반복하여 K개의 초기중점을 설정한다.  이와 같이 초기 중점을 설정할 경우, n+1번째 중점은 n번째 중점에서 가급적 멀리 떨어지는 경향이 발생하며, 이에따라 local optimum에 빠질 확률이 줄어들게 된다.

<center><img src="{{ "/assets/images/K_menas_plus.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

> $$
> \begin{align*}
> 거리\;확률분포\;=&\; \frac{d(x_i,\mu_j)^2}{\sum_{i=1}^nd(x_i,\mu_j)^2}\;\;\\
> \;\\
> =&\left[\frac{3^2}{3^2+4^2+5^2},\frac{16}{50},\frac{25}{50}\right]\;\;(그림상의\;초기중점\;\mu_1 \to \mu_2)
> \;\\
> \;\\
> =&[0.18,0.32,0.5]\;\;\;(해당\;확률분포에서\;1개\;샘플링)\\
> \;\\
> =& [0,0,1]\;\;\;(x_4번이\;샘플림됨,\;\mu_2 = x_4)\\
> \;\\
> \\&해당\;작업을\;j=K-1까지\;반복
> \end{align*}\\
> $$



### Practice (python)

[Blobs (K-means))](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/K_means/K-Means(blobs).py)

[Blobs (K-means++)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/K_means/K-Means(plus_blobs).py)

[MNIST (K-means++)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/K_means/K-Means(plus_mnist).py)

[StockPattern (K-means++)](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/K_means/K-Means(plus_stockPattern).py)


---
layout: post
title: XGBoost
description: Extreme Gradient Boosting
image:
use_math: true
ml: True
---

> - Esemble - Boosting
> - 잔차(residual) 학습

### XGBOOST 작동 원리

[Gradient Boosting](https://hyunjun-bruce-lee.github.io/2021/12/06/Gradient-Boosting.html)의 경우 leaf node의 평균값을 이용하여 residual을 줄여 나갔으나, XGBOOST는 loss함수가 최소가 되는 최적값을 찾아 residual을 줄여간다.

이와 더불어 XGBOOST에서는 regularization(규제)와 pruning(가지치기)을 통해 overfitting을 줄이고 일반화 특성을 좋게 만든다.

XGBOOST는 Gradient Boost보다 메모리 효율이 좋고, 속도도 빠르며, 대용량 데이터 처리에 성능이 우수하다고 알려져있다.

&nbsp;

### XGBOOST 상세 학습 과정

아래의 데이터가 주어졌을경우 XGBOOST는 아래의 식을 토대로 다음(1~6)과 같은 방법으로 학습을 수행한다.($\gamma는 본예시에서는 고려하지 않는다$)

<center><img src="{{ "/assets/images/XGBOOST_1.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

1 - 초기 추정치를 임의로 설정(ex: 0.5)하고, residual (0)을 계산한다.

<center><img src="{{ "/assets/images/XGBOOST_2.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

2 - residual(0)에 대해 similarity를 계산한다.($\lambda\;=\;1$)

> $$
> \begin{align*}
> similarity\;&=\;\frac{(-10.5+6.5+7.5-7.5)^2}{4+1}\;=\;3.2\\
> \end{align*}
> $$

3 - residual(0)에 대해 tree를 생성한다. 이때 tree의 분기는 분할 전,후의 similarity score를 기준으로 계산한 gain을 기준으로 판단한다. $(split\;if\;argmax(gain)\;>0)$

<center><img src="{{ "/assets/images/XGBOOST_3.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

4 - 확정된 tree의 output value를 계산하고, 이를 이용하여 residual(1)을 계산한다.(학습률($\epsilon$)\;=\;0.3)

<center><img src="{{ "/assets/images/XGBOOST_4.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

5 - residual(1)을 이용하여 새로운 tree를 생성하고, 위의 작업을 residual이 충분히 줄어들때까지 반복한다. 

훈련이 종료된 뒤 신규 데이터에 대한 추정은 다음과 같이 이루어진다. ($if\;num(tree)\;=\;n$)
$$
\hat{y}\;=\;\hat{y}_{initial}\;+\;(\epsilon\;\times\;output_{tree_1})\;+\;(\epsilon\;\times\;output_{tree_2})\;+\;\cdots\;+\;(\epsilon\;\times\;output_{tree_n})
$$


### Practice (Python)

[Boston Data](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/XGBoost/XGBoost(regression).py)

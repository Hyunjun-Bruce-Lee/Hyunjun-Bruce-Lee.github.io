---
layout: post
title: XGBoost
description: Extreme Gradient Boosting
image:
use_math: true
ml: True
---

> - Esemble - Boosting
> - 잔차(residual) 학습

### XGBOOST 작동 원리

[Gradient Boosting](https://hyunjun-bruce-lee.github.io/2021/12/06/Gradient-Boosting.html)의 경우 leaf node의 평균값을 이용하여 residual을 줄여 나갔으나, XGBOOST는 loss함수가 최소가 되는 최적값을 찾아 residual을 줄여간다.

이와 더불어 XGBOOST에서는 regularization(규제)와 pruning(가지치기)을 통해 overfitting을 줄이고 일반화 특성을 좋게 만든다.

XGBOOST는 Gradient Boost보다 메모리 효율이 좋고, 속도도 빠르며, 대용량 데이터 처리에 성능이 우수하다고 알려져있다.

&nbsp;

### XGBOOST 상세 학습 과정(regression)

아래의 데이터가 주어졌을경우 XGBOOST는 아래의 식을 토대로 다음(1~5)과 같은 방법으로 학습을 수행한다.($\gamma는 본예시에서는 고려하지 않는다$)

<center><img src="{{ "/assets/images/XGBOOST/XGBOOST_1.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

1 - 초기 추정치를 임의로 설정(ex: 0.5)하고, residual (0)을 계산한다.

<center><img src="{{ "/assets/images/XGBOOST/XGBOOST_2.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

2 - residual(0)에 대해 similarity를 계산한다.($\lambda\;=\;1$)

> $$
> \begin{align*}
> similarity\;&=\;\frac{(-10.5+6.5+7.5-7.5)^2}{4+1}\;=\;3.2\\
> \end{align*}
> $$

3 - residual(0)에 대해 tree를 생성한다. 이때 tree의 분기는 분할 전,후의 similarity score를 기준으로 계산한 gain을 기준으로 판단한다. $(split\;if\;argmax(gain)\;>0)$

<center><img src="{{ "/assets/images/XGBOOST/XGBOOST_3.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

4 - 확정된 tree의 output value를 계산하고, 이를 이용하여 residual(1)을 계산한다.(학습률($\epsilon$)\;=\;0.3)

<center><img src="{{ "/assets/images/XGBOOST/XGBOOST_4.PNG" | absolute_url }}" width = 'auto' height = 'auto' alt="" /></center>

5 - residual(1)을 이용하여 새로운 tree를 생성하고, 위의 작업을 residual이 충분히 줄어들때까지 반복한다. 

훈련이 종료된 뒤 신규 데이터에 대한 추정은 다음과 같이 이루어진다. ($if\;num(tree)\;=\;n$)

> $$
> \hat{y}\;=\;\hat{y}_{initial}\;+\;(\epsilon\;\times\;output_{tree_1})\;+\;(\epsilon\;\times\;output_{tree_2})\;+\;\cdots\;+\;(\epsilon\;\times\;output_{tree_n})
> $$

&nbsp;

### Practice (Python)

[Boston Data](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/XGBoost/XGBoost(regression).py)

&nbsp;

&nbsp;

&nbsp;

### XGBOOST 상세 학습 과정(classification)

XGBoost에서의 Classification(분류)문제의 경우 Gradient Boosting의 Classification 문제와 같이 odds,log(odds),probability의 개념을 사용하고, 2진 분류의 경우 BCE(Binary Cross Entropy)를, 다중분류의 경우 Softmax Objective Function을 loss function으로 이용한다.

Loss function이 달라짐에 따라 output value와 similarity에 대한 계산 식이 다음과 같이 변경된다.

> $$
> \begin{align*}
> output\;value(O)\;&=\;\frac{\sum_{1=1}^nresidual}{\sum_{1=1}^np_i(1-p_i)}\;\;\;(p\;:\;probability)\\
> \;\\
> similarity\;score\;&=\;\frac{(\sum_{1=1}^nresidual_i)^2}{\sum_{1=1}^np_i(1-p_i)}
> \end{align*}
> $$

이외의 과정은 동일하나, 훈련이 종료된후 신규 데이터에 대한 추정치를 계산할 시 계산되는 최종 값을 그대로 이용하는것이 아닌 sigmoid 함수에 최종 값을 적용시켜 범주형으로 변환하여 최종 분류 예측치를 도출한다.

&nbsp;

### Practice (Python)

[Cancer Data](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/XGBoost/XGBoost(classification).py)

[Iris Data](https://github.com/Hyunjun-Bruce-Lee/ML_study/blob/master/XGBoost/XGBoost(classification-2).py)

